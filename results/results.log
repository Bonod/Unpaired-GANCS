4/25 1908
exp1_b5000: looks good because first 5000 batches are training with mse_factor=1
exp1_b10000: mse_factor decreased from 1 to 0.96, already looks worse than at b5000. GAN loss takes more time to converge. Will train further from the checkpoint if resources allow.

4/26 2039
For exp3: Batch [10] G_Loss[1.2340922355651855], G_mse_Loss[0.240], G_LS_Loss[1.234], D_Real_Loss[0.785], D_Fake_Loss[4.447]
Batch [34710] G_Loss[0.0032694723922759295], G_mse_Loss[8.998], G_LS_Loss[0.003], D_Real_Loss[0.058], D_Fake_Loss[0.912]
Batch [40000], G_Loss[0.06962931901216507], G_mse_Loss[16.926], G_LS_Loss[0.070], D_Real_Loss[0.072], D_Fake_Loss[0.544]

5/1 1630
For exp5: exp5*.png gets better, but all illustrate noisy vertical lines, at 6000 grid became significant

5/2 1411
For exp6: ran 3x faster even with 4x larger batch size, 3h 20k batches. 
For exp7: similar loss trend as exp6, disc_fake plateaus at 10k batch.

5/4:
check test time data consistency: 1.2e-09
export train example from exp7 batch 47k: as bad as test
try 3k pure mse and 0.1 mse after 3.2k (exp7): paused after ~7k
exp8 and exp9 converged

5/5:
3k pure mse then pure GAN for unmasked: 
try LReLU and batch size of 4
